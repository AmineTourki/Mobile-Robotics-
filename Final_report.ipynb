{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ff16be-b75d-4bab-9167-c2c45d300ffa",
   "metadata": {},
   "source": [
    "*Report Basics of Mobile Robotics*  \n",
    "## Students: \n",
    "- Levy Alexandre \n",
    "- Tourki Amine \n",
    "- Tourki Emna \n",
    "- Zghari Aya\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07319473-9f87-4067-8566-ea967919f7c0",
   "metadata": {},
   "source": [
    "Link to our [Github](https://github.com/AmineTourki/Mobile-Robotics-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a99af2c2-81ac-4e9c-80d7-0429082658ed",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59968578-4f00-4dad-8c35-96920a9076d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from A_star_algorithm import run_A_Star\n",
    "from vision_agent import Vision\n",
    "import local_navigation\n",
    "import navigation\n",
    "import utils\n",
    "import  Kalman\n",
    "\n",
    "%matplotlib inline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1bf413-0f4a-44e5-ab78-d5972213dfd8",
   "metadata": {},
   "source": [
    "**Introduction**  \n",
    "This project aims to make a Thymio navigate in a course where it will have to use Computer Vision, Global and Local Navigation with a filter to reduce positioning errors.\n",
    "\n",
    "The arena that we present you is modular, the obstacles can change place and the start and goal can be completely different. In this arena we also force our robot to pass through a checkpoint, which will spice up the route it has to make."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad93698-472b-4be3-bcf8-50ed8256e11e",
   "metadata": {},
   "source": [
    "## Method Overview\n",
    "\n",
    "\n",
    "\n",
    "The camera computes a global map from images captured from the webcam. Global navigation works with the A* algorithm. The Thymio first goes to the checkpoint in green and then to the final point in red.  If unexpected obstacles are encountered on the path of the THymio, local avoidance is triggered to avoid the obstacle ahead with proximity sensors and a replanning is sent. A kalman filter is used to reduce the noise and have a perfect trajectory. \n",
    "\n",
    "<img src=\"Images/Schema_project.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d33d7-b761-4a80-aa57-d39f4aeb422d",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325f061-c856-4cf4-955d-06f984c0788d",
   "metadata": {},
   "source": [
    "## Map Setup\n",
    "\n",
    "<img src=\"Images/Setup.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3080c-27a7-41d6-808a-1bd7a5afb7dc",
   "metadata": {},
   "source": [
    "## Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ea71e-f236-44ef-86c9-4fd1dab139ae",
   "metadata": {},
   "source": [
    "The vision Class is strongly dependent on the map data, which can be updated continuously from the video. The construction of a global map consists of the following steps:\n",
    "\n",
    "1. The raw images are sampled at a given frequency from the webcam.\n",
    "2. A rebounding of the arena is done.\n",
    "3. Multiple color masks are applied to the transformed image to distinguish the objectives,the thymio, the arena and obstacles.\n",
    "4. Multiple color masks are applied to the transformed image to distinguish the target position, thymio (front and rear) and obstacles.\n",
    "\n",
    "Note:\n",
    "The rebounding and the localisation of the objectives and the obstacle are done on the first image. The next images are used for the robot position given to the Kalman filter.   \n",
    "The open library CV2 was used for the vision module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e7f86-3c09-425d-8465-ce9ea042df97",
   "metadata": {},
   "source": [
    "- Input\n",
    "    \n",
    "    - Raw image captured from camera\n",
    "    - Distance in real between the two circles on the robot to have a real world scale\n",
    "    - Color HSV values that helps define the map (this is sensitive to light and needs to be calibrated after setting environment)\n",
    "    \n",
    "- Output\n",
    "    - Global map that marks all occupancy with 1 and vacanncy with 0\n",
    "    - Starting pose, checkpoint pose and finish pose of the Thymio\n",
    "\n",
    "- Limitations \n",
    "    - The color masks are rather sensitive to lighting conditions and hyperparameters had to be calibrated\n",
    "    \n",
    "- Keyparameters\n",
    "    - Color mask Table\n",
    "        | Color | Used for | Lower Bounds | Upper Bounds |\n",
    "        | --- | --- | --- | --- |\n",
    "        | Blue | Robot Position | [87,97,50] | [164,255,160] |\n",
    "        | Red | Final Destination | [150,70,50] | [190,255,255] |\n",
    "        | Green | Checkpoint | [40,40,40] | [100,255,255] |\n",
    "        | Black | Robot Position | [0,0,0] | [180,255,50] |\n",
    "        | White+Green+Blue+Red | Reboarder Map |  [0,0,40] | [255,255,255] |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba377d8-0613-492b-829b-90ca5f057f79",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Class Vision\n",
    "\n",
    "Here is the class that can be found in vision_agent.py and its initialization:\n",
    "\n",
    "    class Vision_Agent:\n",
    "    def __init__(self):\n",
    "        self.converter = 0\n",
    "        self.cam = cv2.VideoCapture(1)\n",
    "        ret, frame = self.cam.read()  # initialize\n",
    "        self.image = frame\n",
    "        self.resize()                 #resize the image\n",
    "        self.hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV) # convert to HSV\n",
    "        self.get_first_image_cam(self.image)                   # delimit of the image\n",
    "        self.hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        \n",
    "        self.angle = 0              # angle of the robot\n",
    "        self.center_robot = [0,0]   # center of the robot\n",
    "        self.parcours = np.zeros((50,50), np.uint8) # occupancy grid\n",
    "        self.parcours_2_pix=[len(self.image[0])/len(self.parcours[0]),len(self.image)/len(self.parcours)]\n",
    "        self.objective_red = None\n",
    "        self.objective_green = None\n",
    "        self.r_in_pix = 0\n",
    "        self.r_in_real = 6\n",
    "\n",
    "The Object Vision is going to initialise all these values and we are going to call update methods and at any time, there are also methods for visualisation and others returning  coefficient values like the conversion from pixel to a real value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1229518-8474-4fc1-8bf7-75d899368ac5",
   "metadata": {},
   "source": [
    "Here is a photo example to illustrate the vision code : \n",
    "\n",
    "<img src=\"Images/Image13.jpeg\" width=\"220\"/>\n",
    "\n",
    "In order to output a global map, each different object has to be detected. In order to detect the robot and its direction we put two blue rounds on it with different sizes, the small one is pointing in its direction. The black objects are the obstacles. The red represents the destination point. The green represents a checkpoint. \n",
    "\n",
    "#### Methods in the class\n",
    "\n",
    "##### resize()\n",
    "\n",
    "Resize the image returns by the camera so that the computation time is smaller. \n",
    "\n",
    "##### read_image()\n",
    "\n",
    "Read a new image from the camera.\n",
    "\n",
    "##### mask_thresh (image, thresh_low, thresh_high)\n",
    "\n",
    "Make a mask of the image in the range specified by the thresholds.\n",
    "\n",
    "#### redefine_mask\n",
    "\n",
    "Apply Morphological Operations so that erases noise in the mask.\n",
    "\n",
    "\n",
    "<img src=\"Images/Denoise.png\" width=\"220\"/>\n",
    "<img src=\"Images/Boom.png\" width=\"220\"/>\n",
    "\n",
    "##### get_robot()\n",
    "\n",
    "Get the position of the robot in the image stored.\n",
    "\n",
    "Here is a photo of the mask after mask_thresh with black and redefine mask for denoising\n",
    "\n",
    "![alt text](Images/Mask_circles.png \"Title\")\n",
    "\n",
    "Then we detect the circles with cv2 HoughCircle and Compute their centers\n",
    "\n",
    "![alt text](Images/Image_Robot.png \"Title\")\n",
    "\n",
    "##### get_obstacles()\n",
    "\n",
    "Get the position of the bounding box of the obstacles and returns an occupancy grid for the A* algorithm\n",
    "\n",
    "\n",
    "\n",
    "Here are photos of the masks after mask_thresh with black and redefine mask for denoising  \n",
    "![alt text](Images/Mask_obstacles.png \"Title\")\n",
    "\n",
    "Then we create a bounding box around each obstacle  \n",
    "![alt text](Images/Image_obstacles.png \"Title\")\n",
    "\n",
    "This parcours is returned as occupancy grid and the obstacles are also dilated so it is easier to dodge them   \n",
    "<img src=\"Images/parcours_a_star.png\" width=\"220\"/>\n",
    "\n",
    "##### get_objectives()\n",
    "\n",
    "Get the center of the objective.\n",
    "\n",
    "Here is a photo of the mask after mask_thresh with green and red and redefine mask for denoising  \n",
    "![alt text](Images/Mask_objective_green.png \"Title\")\n",
    "![alt text](Images/Mask_objective_red.png \"Title\")\n",
    "\n",
    "Then we create a bounding box around each objective and get their center  \n",
    "![alt text](Images/Image_Objectives.png \"Title\")\n",
    "\n",
    "\n",
    "##### print_path()\n",
    "\n",
    "Print the path generated by the A* algorithm for the path planning on the image.  \n",
    "\n",
    "<img src=\"Images/Image50.jpeg\" width=\"220\"/>\n",
    "\n",
    "##### get_first_image_cam()\n",
    "\n",
    "Crop the image so that it fits the arena and store the coordinates of the arena.\n",
    " \n",
    "It first creates a mask of the arena with mask_thresh and morphological Operations.  \n",
    "<img src=\"Images/Image_reboardered.png\" width=\"220\"/>\n",
    "\n",
    "Then we detect the angles of this mask and recompute a reboardered image from the angles\n",
    "\n",
    "##### get_image_cam()\n",
    "\n",
    "Use the coordinates of the arena to crop the Image .\n",
    "\n",
    "##### get_pix_2_real()\n",
    "\n",
    "Returns a factor to convert from pixel value to real value in cm.\n",
    "\n",
    "##### get_grid_2_real ()\n",
    "\n",
    "Returns a factor to convert from grid value to real value in cm.\n",
    "\n",
    "##### update\n",
    "\n",
    "Do  read_image(), get_robot(), get_obstacles(), get_objectives(). All the necessary task to detect all the object on the map.\n",
    "\n",
    "\n",
    "![alt text](Images/Image.png \"Title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63049cbc-1df6-4a0b-8282-066adaf90b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "while(True):\n",
    "    Vision.update()\n",
    "    cv2.imshow('Show', Vision.image)   \n",
    "    if cv2.waitKey(1) == ord(\"q\"):\n",
    "        break\n",
    "        \n",
    "Vision.cam.realease()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1ad5e-2a86-4ac6-b666-c9167e494324",
   "metadata": {},
   "source": [
    "## Global navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bab49-9f03-4988-a122-e0196a04b28c",
   "metadata": {},
   "source": [
    "For the purpose of navigation, we created a clas that represents the robot. The class can be found in utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856344be-aca4-46aa-aea9-4a87d1abd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******** CONSTANTS ********\n",
    "\n",
    "MOTORSPEED = 50  # Forward speed\n",
    "MOTOR_ROT = 50  # Rotation speed\n",
    "FULLROTATIONTIME = 16.5  # time in seconds to turn 2 pi rad\n",
    "\n",
    "\n",
    "# ******** CLASS ********\n",
    "class MyThymio(object):\n",
    "    \"\"\"\n",
    "    Class representing our Thymio robot, gathering state information and class\n",
    "    methods to modifiy its state, command the motors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, th, x, y, theta):\n",
    "        \"\"\"\n",
    "        Initialisation of the object\n",
    "        :param th: instance linking to the Thymio robot connected via USB\n",
    "               x,y: x and y positions of the center of the Thymio\n",
    "               theta: the absolute angle orientation of the Thymio\n",
    "        :return: creates the object of class MyThymio\n",
    "        \"\"\"\n",
    "\n",
    "        self.th = th\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.angle = theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd1bb6-33c7-47f2-b6c5-5c7f5e36598e",
   "metadata": {},
   "source": [
    "Here are the methods that we can call on an instance: \n",
    "- def get_pos(self)\n",
    "- def set_pos(self, x, y)\n",
    "- def motor_stop(self)\n",
    "- def motor_forward(self)\n",
    "- def motor_rotate(self, direction)\n",
    "- def get_angle(self)\n",
    "- def set_theta(self, theta)\n",
    "- def get_speed(self)\n",
    "- def get_previous_pos(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ff2b4-9b43-4f5d-ab2a-0b61de1747b9",
   "metadata": {},
   "source": [
    "### A-Star Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617b2d4-b461-4ea5-8f0e-fb12fc16f356",
   "metadata": {},
   "source": [
    "We use the same A* algorithm provied in the TP. It is based on the pseudocode provided in [Wikipedia](https://en.wikipedia.org/wiki/A*_search_algorithm)  \n",
    "The vision module is responsible for calculating the occupancy grid, the size of the grid is modular and can be adjusted in the Vision module. For our project, we fixed a size of 50  by 50 for performance reasons. A 50 by 50 grid takes around 4 seconds to be resolved while a grid of 60 by 60 takes 12 seconds (can vary depending on the power of the computer) that is why we opted for that size.  \n",
    "For a better looking and more smooth navigation, we also opted for 8-connectivity movement (8N)  \n",
    "**Note:** The implementation of the code can be found in the file A_star_algorithm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52403b-91b9-410c-a26b-3a1957b919e7",
   "metadata": {},
   "source": [
    "for the given arena:    \n",
    "![alt text](report_images/A_star_0.jpeg \"Title\")    \n",
    "The vision would generate the following mask:    \n",
    "![alt text](report_images/A_star_1.png \"Title\")    \n",
    "and generate the following grid    \n",
    "![alt text](report_images/A_star_2.png \"Title\")  \n",
    "For the first checkpoint (in green) we generate the following path  \n",
    "![alt text](report_images/visited_green.png \"Title\")  \n",
    "And for the Goal (in red) we obtain the following path:  \n",
    "![alt text](report_images/visited_red.png \"Title\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c806ca-cda8-43c7-8634-8734ac118fb0",
   "metadata": {},
   "source": [
    "The following function was created to directly call the A star algorithm, it only takes as input the Start, the Goal and the occupency grid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880b94b-b264-47a5-9c47-ebc0cc87dc32",
   "metadata": {},
   "source": [
    "```def run_A_Star(occupancy_grid, start, goal):```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549860fd-0035-49fe-844f-22db9579ee5a",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75f97a-46d4-4a7c-81f6-3da93456dc23",
   "metadata": {},
   "source": [
    "### Kalman Filter    \n",
    "Our Kalman filter consists of a prediction step and a correction step. We use simple odometry to estimate the robot position and the camera to correct the robot position. The filter was also designed to work on linear movements only.  \n",
    "The global navigation consists of translation steps and rotations steps done seperately, but we will assume that the rotations do not infer any error in the odometry. This choice was made because the camera is not sensitive enough to detect small rotations and would return false values. We also found that using only the translation is sufficient and the filter performs still well. We also use a constant speed of 50 for the Thymio.\n",
    "The filter takes the position of the robot as well as its orientation as an input vector.  \n",
    "All the calculus was made in centimeters for the robot positions and in radians for the angles  \n",
    "**Note:** the implementation of the Kalman filter can be found in the file Kalman.py\n",
    "\n",
    "$$X = \\begin{bmatrix} x\\\\\n",
    "y\\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We takes an arbitrary covariance matrix with high values as we are uncertain of where the robot starts\n",
    "$$P = \\begin{bmatrix} 100 && 0 && 0\\\\\n",
    "0 && 100 && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{4}\n",
    "\\end{bmatrix}$$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23823a7a-98e7-4302-921c-7731dab6ddca",
   "metadata": {},
   "source": [
    "The odometry uses this dynamic equation:  \n",
    "\n",
    "$$X_{priori}=\\begin{bmatrix} x_{priori}\\\\\n",
    "y_{priori}\\\\\n",
    "\\alpha_{priori}\n",
    "\\end{bmatrix} = \\begin{bmatrix} x_{previous}\\\\\n",
    "y_{previous}\\\\\n",
    "\\alpha_{previous}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix} speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "speed*\\Delta t*cos(\\alpha_{previous})\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e8f3a-2cba-4aa1-98fe-1fa6f170c79f",
   "metadata": {},
   "source": [
    "For the computation of the new covariance matrix, we take the jacobian of the dynamic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ba3a1-329e-4c59-8bef-c8bdc5aca40c",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix} 1 && 0 && -speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "0 && 1 && -speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "0 && 0 && 0\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb3ead-0ff9-476c-8c88-5eb2eaafb974",
   "metadata": {},
   "source": [
    "and we finally add the uncertainty matrix to our covariance matrix. we obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6d19a-8aa9-42a6-ba0f-0494864b8e0b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P_{priori}=APA^{T} + Q\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98973d8-3fe9-47f5-a2c5-1748c093ddd6",
   "metadata": {},
   "source": [
    "For the matrix Q we use the variance of the robot speed to calculate a position variance using sampling, we obtain a value of  $\\sigma_x$ = $\\sigma_y$ = 0.123 cm. The error on the angle is set arbitrarily to $3^{\\circ}$ = $\\frac{\\pi}{60}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283ff66-1dde-435c-843d-bcaa3f91f841",
   "metadata": {},
   "source": [
    "$$Q = \\begin{bmatrix} 0.123 && 0 && 0\\\\\n",
    "0 && 0.123 && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{60}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a79b3-13a6-432d-bfc8-6331cf061fe4",
   "metadata": {},
   "source": [
    "Once the estimation step is complete, we use the update step to correct the robot position using the robot position returned by the camera.  \n",
    "In this step we simply assume the noise R on the camera readings to be equal to $\\sigma$ = 2*pixel size (cm). The value of the pixel size in centimeters is given by the the vision agent. We obtain diffent values of pixel size for the x and y values depending on the size of the Arena we are using. We also approximate the angle error on the camera to $4.5^{\\circ}$ = $\\frac{\\pi}{40}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40468c8e-f532-4009-9ff6-bffaf4ec1805",
   "metadata": {},
   "source": [
    "$$R = \\begin{bmatrix} \\sigma_x && 0 && 0\\\\\n",
    "0 && \\sigma_y && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{40}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64b17a-32db-4adb-bfa6-8922359b7ffe",
   "metadata": {},
   "source": [
    "The measurement matrix H is a unit diagonal matrix as we simply take the values returned by the vision module without modifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def79b1-6252-4877-9400-d1aa52e11419",
   "metadata": {},
   "source": [
    "$$H = \\begin{bmatrix} 1 && 0 && 0\\\\\n",
    "0 && 1 && 0\\\\\n",
    "0 && 0 && 1\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c238f8-616b-4186-886c-620d1aff8666",
   "metadata": {},
   "source": [
    "We then simply calculate the innovation $i$ and the prediction covariance S in order to calculate the Kalman gain K. The equations are as follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f26bc-39bf-49a7-a3e7-afb5cca81bc2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "i = X_{mesure} - HX_{priori}  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "S=HP_{priori}H^{T} + R\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18814af8-14b5-4aae-8583-5125a6802513",
   "metadata": {},
   "source": [
    "The Kalman K gain tells how much the predictions should be corrected based on the measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6d007-69f4-4692-b511-c36b31d9e4c2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "K=P_{priori}H^{T}S^{-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0e852-fb1e-4a85-a46d-94387d534b30",
   "metadata": {},
   "source": [
    "We can then calculate a posterior position estimate and a posterior covariance matrix estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ab318-d469-48da-827b-0b2c502e3413",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X_{post} = X_{mesure} + ki  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "P_{post}=P_{priori} - KHP_{priori}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f1c0b-ceda-4efb-9034-7d540b2592de",
   "metadata": {},
   "source": [
    "Given that the image quality of the camera isn't very high, we can get errors on the measured values at random times. To make up for that we simply discard the measured values by the camera and only use the estimated values in the Kalman filter. A boolean variable (robot_detected) is simply set to True by the vision module in order for the Kalman filter to use the prediction only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edfbb5d-b57e-4432-9572-c28a682b3c50",
   "metadata": {},
   "source": [
    "The final Kalman filter can be called by the function: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58316c-4607-457c-b2c5-cc5bf0c03209",
   "metadata": {},
   "source": [
    "```def kalman_estimate(X_previous,P_previous,delta_t,speed)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becad887-5b19-4b81-8e05-c538208e13d3",
   "metadata": {},
   "source": [
    "it returns the corrected position and angle of the robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ede16-51f3-42c5-ba18-e4cb8ce6f52a",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X_{final}= kalman\\_estimate(X\\_previous,P\\_previous,delta\\_t,speed)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed899a1b-c23d-4983-ab3f-98fd20310049",
   "metadata": {},
   "source": [
    "## Local navigation using proximity sensors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c933f458-7906-4fac-be20-fbbfa7fbd55c",
   "metadata": {},
   "source": [
    "The robot needs to avoid the obstacles unseen by the vision. To do so, the horizontal proximity sensors of the robot are used to detect if there is anything in front of him. The objective is then to rotate in place until the obstacle is not detected with the sensors then to move from the obstacle by going forward for TIME_FORWARD seconds or until another obstacle is detected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6514e9a9-5aee-4940-a892-dfc475f7da2c",
   "metadata": {},
   "source": [
    "**Limitations**\n",
    "- The robot cannot move through an arc of circle but can only move straight forward or turn around the center of its wheels axis. As a result the wall of the obstacle cannot be followed smoothly to find again the optimal path. The only solution to avoid the obstacle is to rotate the robot in place until the obstacle is not detected with the sensors then go forward for a fixed amount of time. This is a limited approach but simple and help us to not lose track of the robot's position by recording the new angle of the robot and the time spent going forward.The angle and the time spent going forward Ts will then be given as input to the function for the kalman filter to estimate the new position of the robot. \n",
    "- The fixed amount of time TIME_FORWARD is set experimentally.If it's too long then the robot can run over fixed obstacles seen by the vision but not from the proximity sensors because the obstacles seen by the vision are black rectangles printed on a paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e564124-f595-4d62-a15d-c7bb94c92e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ******** CONSTANTS ********\n",
    "\n",
    "THRESHOLD_SIDE = 100  # threshold setting the distance to the wall before going away of it\n",
    "THRESHOLD_OBST = 2250  # threshold for entering in local navigation\n",
    "TIME_FORWARD = 2  # time in seconds going away from obstacle before new computation\n",
    "\n",
    "\n",
    "# ******** FUNCTIONS ********\n",
    "\n",
    "def obstacle_avoidance(myRobot, prox_sens):\n",
    "    \"\"\"\n",
    "    The robot avoid the obstacle by turning on himself until the obstacle is not detected\n",
    "    then goes forward\n",
    "    :param\n",
    "        myRobot: our robot\n",
    "        prox_sens: values of the proximity sensors\n",
    "    :return:\n",
    "        Ts: time spent going forward\n",
    "    \"\"\"\n",
    "    # The obstacle is on the left - > rotate right\n",
    "    if (prox_sens[0] + prox_sens[1]) > (prox_sens[4] + prox_sens[3]):\n",
    "        Ts = bypass(\"right\", prox_sens, myRobot)\n",
    "    # The obstacle is on the right - > rotate left\n",
    "    else:\n",
    "        Ts = bypass(\"left\", prox_sens, myRobot)\n",
    "\n",
    "    return Ts\n",
    "\n",
    "\n",
    "def bypass(direction, prox_sens, myRobot):\n",
    "    \"\"\"\n",
    "    Bypass the obstacle by turning on himself until the obstacle is not detected\n",
    "    then goes forward\n",
    "    :param\n",
    "        direction: direction of the rotation\n",
    "         prox_sens: values of the proximity sensors\n",
    "        myRobot: our robot\n",
    "    :return:\n",
    "        Ts: time spent going forward\n",
    "    \"\"\"\n",
    "    reset_timer = 1  # to start the timer only the first time one enter into the loop\n",
    "    \n",
    "    # the robot turn on himself until the sensor don't detect the obstacle\n",
    "    while sum(prox_sens[i] > THRESHOLD_SIDE for i in range(0, 5)) > 0:\n",
    "\n",
    "        myRobot.motor_rotate(direction)\n",
    "\n",
    "        if reset_timer:\n",
    "            t_start = time.perf_counter()\n",
    "            reset_timer = 0\n",
    "\n",
    "        prox_sens = myRobot.th[\"prox.horizontal\"]\n",
    "        if sum(prox_sens[i] > THRESHOLD_SIDE for i in range(0, 5)) == 0:\n",
    "            # Stop the timer\n",
    "            t_stop = time.perf_counter()\n",
    "            turning_time = t_stop - t_start\n",
    "            # Computes the angle variation\n",
    "            dtheta = turning_time * 2 * m.pi / utils.FULLROTATIONTIME\n",
    "            # Update theta\n",
    "            if direction == \"right\":  # Opposite from convention\n",
    "                theta = myRobot.get_angle() + dtheta\n",
    "            else:\n",
    "                theta = myRobot.get_angle() - dtheta\n",
    "\n",
    "            myRobot.set_theta(theta)\n",
    "\n",
    "            # Go forward\n",
    "            Ts = local_forward(myRobot)\n",
    "\n",
    "    return Ts\n",
    "\n",
    "\n",
    "def local_forward(myRobot):\n",
    "    \"\"\"\n",
    "    Go forward for TIME_FORWARD seconds or until another obstacle is detected\n",
    "    :param myRobot: our robot\n",
    "    :return:\n",
    "        Ts: time spent going forward\n",
    "    \"\"\"\n",
    "    # Set motors speed to go forward\n",
    "    myRobot.motor_forward()\n",
    "\n",
    "    # Increment time counter while checking for obstacles\n",
    "    step_forward = TIME_FORWARD / 16\n",
    "    nb_step = 0\n",
    "    for i in range(16):\n",
    "        prox = myRobot.th[\"prox.horizontal\"]\n",
    "        # Break if an obstacle is detected\n",
    "        if sum(prox[i] > THRESHOLD_SIDE for i in range(0, 5)):\n",
    "            break\n",
    "        time.sleep(step_forward)\n",
    "        nb_step = nb_step + 1\n",
    "\n",
    "    myRobot.motor_stop()\n",
    "\n",
    "    # Compute time forward and average speed\n",
    "    Ts = nb_step * step_forward\n",
    "\n",
    "    return Ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dbbde1-9cd9-49cc-8df2-df2eefdd6ff2",
   "metadata": {},
   "source": [
    "## Motor Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c5ceef-36e8-4782-b635-92cdacf1d45d",
   "metadata": {},
   "source": [
    "From the actual estimate of position and the set of points from the optimal path, the function **next_node** set the next node to visit and the function **go_to** adjusts the orientation of the robot by rotating him towards his targeted node then sets the motors to go forward.  \n",
    "\n",
    "*Notes:*\n",
    "\n",
    "- In order to avoid that the robot correct its orientation too often, the next node tracked is taken a few points (STEP_SKIP) ahead of the actual closest point of the optimal path. The drawback of this approach is that the robot could cross an obstacle instead of stricly respecting a planned corner.\n",
    "- In the function **go_to**, we only adjust the orientation of the robot towards his targeted node when the difference between the current angle of Thymio the planned one is higher than $\\pi$/16 to avoid changing the orientation too often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0e094d-93e8-4f35-ac4e-2879a8251dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "STEP_SKIP = 2 \n",
    "\n",
    "def next_node(myRobot, path):\n",
    "    \"\"\"\n",
    "    Set the next node to visit\n",
    "\n",
    "    :param:\n",
    "        thymio: our robot\n",
    "        path: optimal path returned from A* as a list of nodes.\n",
    "        next node. makes the navigation smoother.\n",
    "    :return:\n",
    "        next_node: optimal next node (x,y) to visit\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute distance between Thymio and each position of the optimal path\n",
    "    dist = np.linalg.norm(myRobot.get_pos() - path , axis=1)\n",
    "    # Selects the closest point\n",
    "    step = np.argmin(dist)\n",
    "    \n",
    "    if step + STEP_SKIP < len(path):\n",
    "        _next = path[step + STEP_SKIP]\n",
    "    else:\n",
    "        _next = path[-1]\n",
    "    return _next\n",
    "\n",
    "\n",
    "def go_to(myRobot, next_pos):\n",
    "    \"\"\"\n",
    "    adjust the orientation of the robot by rotating him towards his targeted node then set the motors to go forward\n",
    "    :param:\n",
    "        thymio: our robot\n",
    "        next_node: targeted node to visit next.\n",
    "    \"\"\"\n",
    "    target_vector = next_pos - myRobot.get_pos()  # vector between actual node and next node.\n",
    "    theta_ref = m.atan2(target_vector[1], target_vector[0])  # Vector angle from x-axis\n",
    "    dtheta = utils.normalise_angle(theta_ref - myRobot.get_angle())  # Normalize angle to minimise rotation\n",
    "    \n",
    "    # Only rotate when the difference between the current angle of Thymio the planned one is higher than Pi/16 \n",
    "    if abs(dtheta) > m.pi / 16:\n",
    "        theta = myRobot.get_angle() + dtheta\n",
    "        \n",
    "        # start the rotation\n",
    "        if dtheta < 0:  # Opposite from convention\n",
    "            myRobot.motor_rotate(\"left\")\n",
    "        else:\n",
    "            myRobot.motor_rotate(\"right\")\n",
    "            \n",
    "        # sleep until the rotation is done\n",
    "        time.sleep(abs(utils.FULLROTATIONTIME * dtheta / (2 * m.pi)))\n",
    "        myRobot.motor_stop()\n",
    "        # updata theta\n",
    "        myRobot.set_theta(theta)\n",
    "    \n",
    "    #Go forward\n",
    "    myRobot.motor_forward()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06003a1-8a26-4cad-8156-41033c527f7b",
   "metadata": {},
   "source": [
    "## Switching between global and local navigation \n",
    "We call the function **follow_path** 2 times. The first time, our objective is the first green goal and the second time the objective is the red goal.\n",
    "While going from a start position to the the designated goal, the robot moves along the global optimal path obtained by the function **run_A_Star**.  \n",
    "\n",
    "If a local obstacle is detected with the proximity sensors, the robot enters the local navigation , moves to avoid the obstacle locally, calls the function **callKalman** to estimate his new position and then reconstruct the path to the goal using **run_A_Star** again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f599575-05b6-434f-9915-14f9cb814a08",
   "metadata": {},
   "source": [
    "**Limitations**  \n",
    "- Kidnapping not allowed\n",
    "- To correctly track the robot, it requires a slow speed and a precise model. For this purpose, it is imposed to move the robot only by turning around the center of its wheels axis or going straight forward.\n",
    "- This algorithm needs more room to avoid the obstacle. Here the map is too small for the robot size so we can only make one obstacle avoidance locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b8073-6f0c-4f73-a97d-b30f7283988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def follow_path(myRobot, Vision, path, goal):\n",
    "    \"\"\"\n",
    "    Follow the path util the goal is reached and stop\n",
    "    :param myRobot: our robot\n",
    "    :param Vision:  our vision agent\n",
    "    :param path: the path to follow\n",
    "    :param goal: the goal\n",
    "    \"\"\"\n",
    "    last_state = \"global\"  # robot state \"global\" or \"local\"\n",
    "    next_pos = path[STEP_SKIP]\n",
    "\n",
    "    # Adjust angle and goes forward\n",
    "    go_to(myRobot, next_pos)\n",
    "    # Starts timer\n",
    "    t_start = time.perf_counter()\n",
    "    # Keep going while goal not reached\n",
    "    while np.linalg.norm(myRobot.get_pos() - goal) > EPS_GOAL:\n",
    "\n",
    "        prox_sens = myRobot.th[\"prox.horizontal\"]\n",
    "\n",
    "        # Checking if should be in local or global navigation\n",
    "        if sum([prox_sens[i] > local_navigation.THRESHOLD_OBST for i in range(0, 5)]) < 1:\n",
    "            state = \"global\"  # global navigation\n",
    "\n",
    "        else:\n",
    "            state = \"local\"  # obstacle avoidance\n",
    "\n",
    "        # Global navigation\n",
    "        if state == \"global\":\n",
    "\n",
    "            if last_state == \"global\":\n",
    "                t_stop = time.perf_counter()\n",
    "                Ts = t_stop - t_start\n",
    "                # estimate new position\n",
    "                callKalman(myRobot, Vision, Ts)\n",
    "\n",
    "            next_pos = next_node(myRobot, path)\n",
    "\n",
    "            # adjusts the orientation then goes forward\n",
    "            go_to(myRobot, next_pos)\n",
    "\n",
    "            # Restarts timer\n",
    "            t_start = time.perf_counter()\n",
    "            \n",
    "            last_state = \"global\"\n",
    "\n",
    "        # Local navigation\n",
    "        elif state == \"local\":\n",
    "            # Stops\n",
    "            myRobot.motor_stop()\n",
    "\n",
    "            # If previously in global navigation, computes actual position\n",
    "            if last_state == \"global\":\n",
    "                # Records time forward in global before entering in local\n",
    "                t_stop = time.perf_counter()\n",
    "                Ts = t_stop - t_start\n",
    "                # estimate new position\n",
    "                callKalman(myRobot, Vision, Ts)\n",
    "\n",
    "            # Enters local navigation\n",
    "            Ts = local_navigation.obstacle_avoidance(myRobot, prox_sens)\n",
    "\n",
    "            # estimate new position\n",
    "            callKalman(myRobot, Vision,Ts)\n",
    "\n",
    "            # Reconstruct the path to the goal\n",
    "            path, visitedNodes = run_A_Star(Vision.parcours, myRobot.get_pos(), goal)\n",
    "\n",
    "            last_state = \"local\"\n",
    "\n",
    "    print(\"Goal reached\")\n",
    "    myRobot.motor_stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913344fa-45f6-4c8c-a706-a2285c1073ad",
   "metadata": {},
   "source": [
    "## Connexion to Thymio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f069b-b312-4c9a-8162-60a942fd33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Thymio import Thymio\n",
    "#With cable\n",
    "th = Thymio.serial(port=\"COM7\", refreshing_rate=0.1)\n",
    "#Wireless\n",
    "#th = Thymio.serial(port=\"\\\\.\\COM5\", refreshing_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d7c0e-3758-482b-a95e-fce6a317674d",
   "metadata": {},
   "source": [
    "## Implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268d33a0-9e88-44f1-a0d5-41b2cab3240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get map information and robot initial pose\n",
    "Vision.update()\n",
    "cv2.imwrite(\"image_start.png\",Vision.image)\n",
    "goal=Vision.objective_green\n",
    "path, visitedNodes = run_A_Star(Vision.parcours, Vision.center_robot, goal)\n",
    "Vision.path=path\n",
    "Vision.print_path()\n",
    "x = Vision.center_robot[0]\n",
    "y = Vision.center_robot[1]\n",
    "theta = utils.deg_to_rad(Vision.angle)\n",
    "#Initialise the robot\n",
    "myRobot=utils.MyThymio(th,x,y,theta)\n",
    "th.set_var(\"motor.right.target\",0)\n",
    "th.set_var(\"motor.left.target\",0)\n",
    "#Initialise Kalman\n",
    "Kalman.kalman_init(Vision)\n",
    "\n",
    "# Go to first goal\n",
    "navigation.follow_path(myRobot,Vision,path,goal)\n",
    "\n",
    "goal=Vision.objective_red\n",
    "path, visitedNodes = run_A_Star(Vision.parcours, myRobot.get_pos(), goal)\n",
    "Vision.path=path\n",
    "\n",
    "# Go to final goal\n",
    "navigation.follow_path(myRobot,Vision,path,goal)\n",
    "\n",
    "cv2.imwrite(\"image_end.png\",Vision.image)\n",
    "Vision.cam.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
