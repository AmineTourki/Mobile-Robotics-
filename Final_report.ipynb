{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ff16be-b75d-4bab-9167-c2c45d300ffa",
   "metadata": {},
   "source": [
    "*Report Basics of Mobile Robotics*  \n",
    "## Students: \n",
    "- Levy Alexandre \n",
    "- Tourki Amine \n",
    "- Tourki Emna \n",
    "- Zghari Aya\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07319473-9f87-4067-8566-ea967919f7c0",
   "metadata": {},
   "source": [
    "Link to our [Github](https://github.com/AmineTourki/Mobile-Robotics-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1bf413-0f4a-44e5-ab78-d5972213dfd8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project aims to make a Thymio navigate in a course where it will have to use Computer Vision, Global and Local Navigation with a filter to reduce positioning errors.\n",
    "\n",
    "The arena that we present you is modular, the obstacles can change place and the start and goal can be completely different. In this arena we also force our robot to pass through a checkpoint, which will spice up the route it has to make.The Thymio first goes to the checkpoint in green and then to the final point in red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad93698-472b-4be3-bcf8-50ed8256e11e",
   "metadata": {},
   "source": [
    "## Method Overview \n",
    "![alt text](report_images/Arena_axis.png \"Title\") \n",
    "**Main ideas:**\n",
    "- The \"global obstacles\" are the black 2D rectangles on the ground detected by the vision.\n",
    "- The \"Local obstacles\" are physical 3D obstacles detected by the proximity sensors and will be intoduced while the robot moves along the global path. We chose a cylindrical white obstacle for the presentation.  \n",
    "- Our robot cannot move through an arc of circle.It can only move straight forward or rotate in place. However , the navigation can still be smooth if we use an 8-Direction A_Star path planning and take small steps forward.That's why in this project we used an 8-Direction A_Star path planning and we set a small fixed time step to go forward.  \n",
    "\n",
    "GLOBAL_FORWARD = 0.1  # step time (in seconds) spent going forward in global navigation.  \n",
    "LOCAL_FORWARD = 0.1 # step time (in seconds) spent going forward in local navigation.\n",
    "\n",
    "- We chose a speed of MOTORSPEED = 50 for both the rotation and and the forward movement.\n",
    "- We used a Kalman filter.We use simple odometry to estimate the robot position and the camera to correct the robot position.Thus, the camera will be used continuously.\n",
    "- The global axes are defined as in the image above and the absolute angle of the robot is then defined positive if clockwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d33d7-b761-4a80-aa57-d39f4aeb422d",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325f061-c856-4cf4-955d-06f984c0788d",
   "metadata": {},
   "source": [
    "### Map Setup\n",
    "\n",
    "<img src=\"Images/Setup.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3080c-27a7-41d6-808a-1bd7a5afb7dc",
   "metadata": {},
   "source": [
    "### Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ea71e-f236-44ef-86c9-4fd1dab139ae",
   "metadata": {},
   "source": [
    "The vision Class is strongly dependent on the map data, which can be updated continuously from the video. The construction of a global map consists of the following steps:\n",
    "\n",
    "1. The raw images are sampled at a given frequency from the webcam.\n",
    "2. A rebounding of the arena is done.\n",
    "3. Multiple color masks are applied to the transformed image to distinguish the objectives,the thymio, the arena and obstacles.\n",
    "4. Multiple color masks are applied to the transformed image to distinguish the target position, thymio (front and rear) and obstacles.\n",
    "\n",
    "Note:\n",
    "The rebounding and the localisation of the objectives and the obstacle are done on the first image. The next images are used for the robot position given to the Kalman filter.   \n",
    "The open library CV2 was used for the vision module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e7f86-3c09-425d-8465-ce9ea042df97",
   "metadata": {},
   "source": [
    "- Input\n",
    "    \n",
    "    - Raw image captured from camera\n",
    "    - Distance in real between the two circles on the robot to have a real world scale\n",
    "    - Color HSV values that helps define the map (this is sensitive to light and needs to be calibrated after setting environment)\n",
    "    \n",
    "- Output\n",
    "    - Global map that marks all occupancy with 1 and vacanncy with 0\n",
    "    - Starting pose, checkpoint pose and finish pose of the Thymio\n",
    "\n",
    "- Limitations \n",
    "    - The color masks are rather sensitive to lighting conditions and hyperparameters had to be calibrated\n",
    "    \n",
    "- Keyparameters\n",
    "    - Color mask Table\n",
    "        | Color | Used for | Lower Bounds | Upper Bounds |\n",
    "        | --- | --- | --- | --- |\n",
    "        | Blue | Robot Position | [87,97,50] | [164,255,160] |\n",
    "        | Red | Final Destination | [150,70,50] | [190,255,255] |\n",
    "        | Green | Checkpoint | [40,40,40] | [100,255,255] |\n",
    "        | Black | Robot Position | [0,0,0] | [180,255,50] |\n",
    "        | White+Green+Blue+Red | Reboarder Map |  [0,0,40] | [255,255,255] |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba377d8-0613-492b-829b-90ca5f057f79",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Class Vision\n",
    "\n",
    "Here is the class that can be found in vision_agent.py and its initialization:\n",
    "\n",
    "    class Vision_Agent:\n",
    "    def __init__(self):\n",
    "        self.converter = 0\n",
    "        self.cam = cv2.VideoCapture(1)\n",
    "        ret, frame = self.cam.read()  # initialize\n",
    "        self.image = frame\n",
    "        self.resize()                 #resize the image\n",
    "        self.hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV) # convert to HSV\n",
    "        self.get_first_image_cam(self.image)                   # delimit of the image\n",
    "        self.hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        \n",
    "        self.angle = 0              # angle of the robot\n",
    "        self.center_robot = [0,0]   # center of the robot\n",
    "        self.parcours = np.zeros((50,50), np.uint8) # occupancy grid\n",
    "        self.parcours_2_pix=[len(self.image[0])/len(self.parcours[0]),len(self.image)/len(self.parcours)]\n",
    "        self.objective_red = None\n",
    "        self.objective_green = None\n",
    "        self.r_in_pix = 0\n",
    "        self.r_in_real = 6\n",
    "\n",
    "The Object Vision is going to initialise all these values and we are going to call update methods and at any time, there are also methods for visualisation and others returning  coefficient values like the conversion from pixel to a real value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1229518-8474-4fc1-8bf7-75d899368ac5",
   "metadata": {},
   "source": [
    "Here is a photo example to illustrate the vision code : \n",
    "\n",
    "<img src=\"Images/Image13.jpeg\" width=\"220\"/>\n",
    "\n",
    "In order to output a global map, each different object has to be detected. In order to detect the robot and its direction we put two blue rounds on it with different sizes, the small one is pointing in its direction. The black objects are the obstacles. The red represents the destination point. The green represents a checkpoint. \n",
    "\n",
    "#### Methods in the class\n",
    "\n",
    "##### resize()\n",
    "\n",
    "Resize the image returns by the camera so that the computation time is smaller. \n",
    "\n",
    "##### read_image()\n",
    "\n",
    "Read a new image from the camera.\n",
    "\n",
    "##### mask_thresh (image, thresh_low, thresh_high)\n",
    "\n",
    "Make a mask of the image in the range specified by the thresholds.\n",
    "\n",
    "#### redefine_mask\n",
    "\n",
    "Apply Morphological Operations so that erases noise in the mask.\n",
    "\n",
    "\n",
    "<img src=\"Images/Denoise.png\" width=\"220\"/>\n",
    "<img src=\"Images/Boom.png\" width=\"220\"/>\n",
    "\n",
    "##### get_robot()\n",
    "\n",
    "Get the position of the robot in the image stored.\n",
    "\n",
    "Here is a photo of the mask after mask_thresh with black and redefine mask for denoising\n",
    "\n",
    "![alt text](Images/Mask_circles.png \"Title\")\n",
    "\n",
    "Then we detect the circles with cv2 HoughCircle and Compute their centers\n",
    "\n",
    "![alt text](Images/Image_Robot.png \"Title\")\n",
    "\n",
    "##### get_obstacles()\n",
    "\n",
    "Get the position of the bounding box of the obstacles and returns an occupancy grid for the A* algorithm\n",
    "\n",
    "\n",
    "\n",
    "Here are photos of the masks after mask_thresh with black and redefine mask for denoising  \n",
    "![alt text](Images/Mask_obstacles.png \"Title\")\n",
    "\n",
    "Then we create a bounding box around each obstacle  \n",
    "![alt text](Images/Image_obstacles.png \"Title\")\n",
    "\n",
    "This parcours is returned as occupancy grid and the obstacles are also dilated so it is easier to dodge them   \n",
    "<img src=\"Images/parcours_a_star.png\" width=\"220\"/>\n",
    "\n",
    "##### get_objectives()\n",
    "\n",
    "Get the center of the objective.\n",
    "\n",
    "Here is a photo of the mask after mask_thresh with green and red and redefine mask for denoising  \n",
    "![alt text](Images/Mask_objective_green.png \"Title\")\n",
    "![alt text](Images/Mask_objective_red.png \"Title\")\n",
    "\n",
    "Then we create a bounding box around each objective and get their center  \n",
    "![alt text](Images/Image_Objectives.png \"Title\")\n",
    "\n",
    "\n",
    "##### print_path()\n",
    "\n",
    "Print the path generated by the A* algorithm on the image.  \n",
    "\n",
    "<img src=\"Images/Image50.jpeg\" width=\"220\"/>\n",
    "\n",
    "##### get_first_image_cam()\n",
    "\n",
    "Crop the image so that it fits the arena and store the coordinates of the arena.\n",
    " \n",
    "It first creates a mask of the arena with mask_thresh and morphological Operations.  \n",
    "<img src=\"Images/Image_reboardered.png\" width=\"220\"/>\n",
    "\n",
    "Then we detect the angles of this mask and recompute a reboardered image from the angles\n",
    "\n",
    "##### get_image_cam()\n",
    "\n",
    "Use the coordinates of the arena to crop the Image .\n",
    "\n",
    "##### get_pix_2_real()\n",
    "\n",
    "Returns a factor to convert from pixel value to real value in cm.\n",
    "\n",
    "##### get_grid_2_real ()\n",
    "\n",
    "Returns a factor to convert from grid value to real value in cm.\n",
    "\n",
    "##### update\n",
    "\n",
    "Do  get_robot(), get_obstacles(), get_objectives(). All the necessary task to detect all the object on the map.\n",
    "\n",
    "\n",
    "![alt text](Images/Image.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1ad5e-2a86-4ac6-b666-c9167e494324",
   "metadata": {},
   "source": [
    "## Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bab49-9f03-4988-a122-e0196a04b28c",
   "metadata": {},
   "source": [
    "To facilitate the navigation, we created a class that represents the robot and control its motors. The class can be found in utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856344be-aca4-46aa-aea9-4a87d1abd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyThymio(object):\n",
    "\n",
    "    def __init__(self, th, x, y, theta):\n",
    "        \"\"\"\n",
    "        Initialisation of the object\n",
    "        :param th: instance linking to the Thymio robot connected via USB\n",
    "               x,y: x and y positions of the center of the Thymio\n",
    "               theta: the absolute angle orientation of the Thymio\n",
    "        :return: creates the object of class MyThymio\n",
    "        \"\"\"\n",
    "\n",
    "        self.th = th\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.angle = normalise_angle(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd1bb6-33c7-47f2-b6c5-5c7f5e36598e",
   "metadata": {},
   "source": [
    "Here are the methods that we can call on an instance: \n",
    "- def get_pos(self)\n",
    "- def set_pos(self, x, y)\n",
    "- def motor_stop(self)\n",
    "- def motor_forward(self, time_forward)\n",
    "- def motor_rotate(self, dtheta)\n",
    "- def get_angle(self)\n",
    "- def set_angle(self, theta)\n",
    "- def get_previous_pos(self)\n",
    "\n",
    "*Notes:*  \n",
    "\n",
    "- The method **motor_rotate(self, dtheta)** sets the speed of the motors to Â±MOTORSPEED to make the robot turn on himself dtheta rad then updates self.angle. It takes into consideration the sign of dtheta. If dtheta positive/ negative, the robot turns right/left.\n",
    "- The method **motor_forward(self, time_forward)** sets the speed of both motors to MOTORSPEED  for time_forward seconds.However this method don't update the positions x,y of the instance self. x,y of the instance self are only updated after estimating the true positions with the Kalman filter.They are updated each time the function **callKalman** is called.The definition of this function **callKalman** is in the file navigation.py\n",
    "- The method **get_previous_pos(self)** is used in the function **callKalman**. Since x and y are only updated just after calling kalman, they are the previous positions of our robot when calling it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c790d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "832ff2b4-9b43-4f5d-ab2a-0b61de1747b9",
   "metadata": {},
   "source": [
    "### A-Star Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617b2d4-b461-4ea5-8f0e-fb12fc16f356",
   "metadata": {},
   "source": [
    "We use the same A* algorithm provied in the TP. It is based on the pseudocode provided in [Wikipedia](https://en.wikipedia.org/wiki/A*_search_algorithm)  \n",
    "The vision module is responsible for calculating the occupancy grid, the size of the grid is modular and can be adjusted in the Vision module. For our project, we fixed a size of 50  by 50 for performance reasons. A 50 by 50 grid takes around 4 seconds to be resolved while a grid of 60 by 60 takes 12 seconds (can vary depending on the power of the computer) that is why we opted for that size.  \n",
    "For a better looking and more smooth navigation, we also opted for 8-connectivity movement (8N)  \n",
    "**Note:** The implementation of the code can be found in the file A_star_algorithm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52403b-91b9-410c-a26b-3a1957b919e7",
   "metadata": {},
   "source": [
    "for the given arena:    \n",
    "![alt text](report_images/A_star_0.jpeg \"Title\")    \n",
    "The vision would generate the following mask:    \n",
    "![alt text](report_images/A_star_1.png \"Title\")    \n",
    "and generate the following grid    \n",
    "![alt text](report_images/A_star_2.png \"Title\")  \n",
    "For the first checkpoint (in green) we generate the following path  \n",
    "![alt text](report_images/visited_green.png \"Title\")  \n",
    "And for the Goal (in red) we obtain the following path:  \n",
    "![alt text](report_images/visited_red.png \"Title\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c806ca-cda8-43c7-8634-8734ac118fb0",
   "metadata": {},
   "source": [
    "The following function was created to directly call the A star algorithm, it only takes as input the Start, the Goal and the occupency grid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880b94b-b264-47a5-9c47-ebc0cc87dc32",
   "metadata": {},
   "source": [
    "```def run_A_Star(occupancy_grid, start, goal):```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21eaaa7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b4421c4",
   "metadata": {},
   "source": [
    "### Global navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08588e6a",
   "metadata": {},
   "source": [
    "The main loop (see implementation at the end of the report) follows this pseudo code:  \n",
    "\n",
    "Repeat  \n",
    "- construct path using function **run_A_Star**\n",
    "- follow that path using function **navigation.follow_path**. Two possibilities to exit **the function follow path**:\n",
    "    - First one: The robot follow the path until the end. The goal is reached. Exit **navigation.follow_path** with return True.\n",
    "    - Second one: When following the path, a local obstacle is detected with the proximity sensors. The robot enters the local navigation , moves to avoid the obstacle locally then exit **navigation.follow_path** with return False to construct a new path with the A_Star.\n",
    "- If first goal objective_green reached (checkpoint goal), change goal to objective_red (final destination)\n",
    "- If red goal is reached, break the loop\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf2835",
   "metadata": {},
   "source": [
    "Here are the function **navigation.follow_path** and the functions related to it. The definition of these function are imported from  the file navigation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba960848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def closest_node_index(current_pos, path, old_index):\n",
    "    \"\"\"\n",
    "    return the closest node of the path to the current position of the robot\n",
    "    and if the robot should change his orientation\n",
    "\n",
    "    :param current_pos: current position of the robot\n",
    "    :param path: array containing the optimal path to follow\n",
    "    :param old_index: index taken in the last call of this function\n",
    "    :return:    closestnode_index (int)\n",
    "                change_angle (bool)\n",
    "\n",
    "    \"\"\"\n",
    "    # check the nodes from old index and ahead and get the index of the closest node to our current position\n",
    "    closestnode_index = old_index + np.linalg.norm(current_pos - path[old_index:], axis=1).argmin()\n",
    "    # only change the orientation if the robot is closest to one of his next nodes compared to the initial node\n",
    "    # don't change angle if already close to goal (goal index ==len(path) - 1)\n",
    "    change_angle = (closestnode_index != old_index) and (closestnode_index != len(path) - 1)\n",
    "    return change_angle, closestnode_index\n",
    "\n",
    "\n",
    "def change_orientation(myRobot, next_pos):\n",
    "    \"\"\"\n",
    "    adjust the orientation of the robot by rotating him towards his next_pos\n",
    "    :param myRobot: our robot, instance of Class MyThymio\n",
    "    :param next_pos: coordinate of the next position\n",
    "    \"\"\"\n",
    "    target_vector = next_pos - myRobot.get_pos()\n",
    "    dtheta = math.atan2(target_vector[1], target_vector[0]) - myRobot.get_angle()\n",
    "    # rotate the robot with angle dtheta\n",
    "    myRobot.motor_rotate(dtheta)\n",
    "\n",
    "\n",
    "def follow_path(myRobot, Vision, path):\n",
    "    \"\"\"\n",
    "    Follow the path until the goal is reached or until the robot enters local navigation\n",
    "    :param myRobot: our robot, instance of Class MyThymio\n",
    "    :param Vision: our vision agent, instance of Class Vision_Agent\n",
    "    :param path: array containing the optimal path to follow\n",
    "    :return:    True if the goal is reached\n",
    "                False if the robot entered local navigation (goal not reached)\n",
    "    \"\"\"\n",
    "    closest_index = 0\n",
    "    first_step = 1\n",
    "    while math.dist(myRobot.get_pos(), path[-1]) > EPS_GOAL:  # path[-1] is the goal\n",
    "        change_angle, closest_index = closest_node_index(myRobot.get_pos(), path, closest_index)\n",
    "        # only change the orientation if the robot is closest to his next node compared to the initial node\n",
    "        if change_angle or first_step:\n",
    "            first_step = 0\n",
    "            next_pos = path[closest_index + 1]\n",
    "            # rotate to face the new target node next_pos\n",
    "            change_orientation(myRobot, next_pos)\n",
    "        # go forward GLOBAL_FORWARD(0.1) seconds\n",
    "        myRobot.motor_forward(GLOBAL_FORWARD)\n",
    "        # estimate new position and update myRobot pose\n",
    "        callKalman(myRobot, Vision, GLOBAL_FORWARD)\n",
    "        # enter local navigation if obstacle detected\n",
    "        entered_local_navigation = avoid_obstacle(myRobot, Vision)\n",
    "        if entered_local_navigation:\n",
    "            # if the robot entered local navigation , return to the main implementation loop to construct a new path\n",
    "            return False\n",
    "\n",
    "    print(\"Goal reached\")\n",
    "    myRobot.motor_stop()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859dee0",
   "metadata": {},
   "source": [
    "*Notes:*  \n",
    "\n",
    "- In the function **closest_node_index(current_pos, path, old_index)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1abb190",
   "metadata": {},
   "source": [
    "### Local navigation using proximity sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1931bd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "549860fd-0035-49fe-844f-22db9579ee5a",
   "metadata": {},
   "source": [
    "## Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75f97a-46d4-4a7c-81f6-3da93456dc23",
   "metadata": {},
   "source": [
    "### Kalman Filter    \n",
    "Our Kalman filter consists of a prediction step and a correction step. We use simple odometry to estimate the robot position and the camera to correct the robot position. The filter was also designed to work on linear movements only.  \n",
    "The global navigation consists of translation steps and rotations steps done seperately, but we will assume that the rotations do not infer any error in the odometry. This choice was made because the camera is not sensitive enough to detect small rotations and would return false values. We also found that using only the translation is sufficient and the filter performs still well. We also use a constant speed of 50 for the Thymio.\n",
    "The filter takes the position of the robot as well as its orientation as an input vector.  \n",
    "All the calculus was made in centimeters for the robot positions and in radians for the angles  \n",
    "**Note:** the implementation of the Kalman filter can be found in the file Kalman.py\n",
    "\n",
    "$$X = \\begin{bmatrix} x\\\\\n",
    "y\\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We takes an arbitrary covariance matrix with high values as we are uncertain of where the robot starts\n",
    "$$P = \\begin{bmatrix} 100 && 0 && 0\\\\\n",
    "0 && 100 && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{4}\n",
    "\\end{bmatrix}$$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23823a7a-98e7-4302-921c-7731dab6ddca",
   "metadata": {},
   "source": [
    "The odometry uses this dynamic equation:  \n",
    "\n",
    "$$X_{priori}=\\begin{bmatrix} x_{priori}\\\\\n",
    "y_{priori}\\\\\n",
    "\\alpha_{priori}\n",
    "\\end{bmatrix} = \\begin{bmatrix} x_{previous}\\\\\n",
    "y_{previous}\\\\\n",
    "\\alpha_{previous}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix} speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "speed*\\Delta t*cos(\\alpha_{previous})\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e8f3a-2cba-4aa1-98fe-1fa6f170c79f",
   "metadata": {},
   "source": [
    "For the computation of the new covariance matrix, we take the jacobian of the dynamic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ba3a1-329e-4c59-8bef-c8bdc5aca40c",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix} 1 && 0 && -speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "0 && 1 && -speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "0 && 0 && 0\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb3ead-0ff9-476c-8c88-5eb2eaafb974",
   "metadata": {},
   "source": [
    "and we finally add the uncertainty matrix to our covariance matrix. we obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6d19a-8aa9-42a6-ba0f-0494864b8e0b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P_{priori}=APA^{T} + Q\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98973d8-3fe9-47f5-a2c5-1748c093ddd6",
   "metadata": {},
   "source": [
    "For the matrix Q we use the variance of the robot speed to calculate a position variance using sampling, we obtain a value of  $\\sigma_x$ = $\\sigma_y$ = 0.123 cm. The error on the angle is set arbitrarily to $3^{\\circ}$ = $\\frac{\\pi}{60}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283ff66-1dde-435c-843d-bcaa3f91f841",
   "metadata": {},
   "source": [
    "$$Q = \\begin{bmatrix} 0.123 && 0 && 0\\\\\n",
    "0 && 0.123 && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{60}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a79b3-13a6-432d-bfc8-6331cf061fe4",
   "metadata": {},
   "source": [
    "Once the estimation step is complete, we use the update step to correct the robot position using the robot position returned by the camera.  \n",
    "In this step we simply assume the noise R on the camera readings to be equal to $\\sigma$ = 2*pixel size (cm). The value of the pixel size in centimeters is given by the the vision agent. We obtain diffent values of pixel size for the x and y values depending on the size of the Arena we are using. We also approximate the angle error on the camera to $4.5^{\\circ}$ = $\\frac{\\pi}{40}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40468c8e-f532-4009-9ff6-bffaf4ec1805",
   "metadata": {},
   "source": [
    "$$R = \\begin{bmatrix} \\sigma_x && 0 && 0\\\\\n",
    "0 && \\sigma_y && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{40}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64b17a-32db-4adb-bfa6-8922359b7ffe",
   "metadata": {},
   "source": [
    "The measurement matrix H is a unit diagonal matrix as we simply take the values returned by the vision module without modifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def79b1-6252-4877-9400-d1aa52e11419",
   "metadata": {},
   "source": [
    "$$H = \\begin{bmatrix} 1 && 0 && 0\\\\\n",
    "0 && 1 && 0\\\\\n",
    "0 && 0 && 1\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c238f8-616b-4186-886c-620d1aff8666",
   "metadata": {},
   "source": [
    "We then simply calculate the innovation $i$ and the prediction covariance S in order to calculate the Kalman gain K. The equations are as follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f26bc-39bf-49a7-a3e7-afb5cca81bc2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "i = X_{mesure} - HX_{priori}  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "S=HP_{priori}H^{T} + R\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18814af8-14b5-4aae-8583-5125a6802513",
   "metadata": {},
   "source": [
    "The Kalman K gain tells how much the predictions should be corrected based on the measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6d007-69f4-4692-b511-c36b31d9e4c2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "K=P_{priori}H^{T}S^{-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0e852-fb1e-4a85-a46d-94387d534b30",
   "metadata": {},
   "source": [
    "We can then calculate a posterior position estimate and a posterior covariance matrix estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ab318-d469-48da-827b-0b2c502e3413",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X_{post} = X_{mesure} + ki  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "P_{post}=P_{priori} - KHP_{priori}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f1c0b-ceda-4efb-9034-7d540b2592de",
   "metadata": {},
   "source": [
    "Given that the image quality of the camera isn't very high, we can get errors on the measured values at random times. To make up for that we simply discard the measured values by the camera and only use the estimated values in the Kalman filter. A boolean variable (robot_detected) is simply set to True by the vision module in order for the Kalman filter to use the prediction only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edfbb5d-b57e-4432-9572-c28a682b3c50",
   "metadata": {},
   "source": [
    "The final Kalman filter can be called by the function: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58316c-4607-457c-b2c5-cc5bf0c03209",
   "metadata": {},
   "source": [
    "```def kalman_estimate(X_previous,P_previous,delta_t,speed)```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becad887-5b19-4b81-8e05-c538208e13d3",
   "metadata": {},
   "source": [
    "it returns the corrected position and angle of the robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1ede16-51f3-42c5-ba18-e4cb8ce6f52a",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X_{final}= kalman\\_estimate(X\\_previous,P\\_previous,delta\\_t,speed)\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed899a1b-c23d-4983-ab3f-98fd20310049",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "474b5dad",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eff52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from vision_agent import Vision_Agent\n",
    "import local_navigation\n",
    "from A_star_algorithm import run_A_Star\n",
    "import navigation\n",
    "import utils\n",
    "import  Kalman\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913344fa-45f6-4c8c-a706-a2285c1073ad",
   "metadata": {},
   "source": [
    "## Connexion to Thymio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f069b-b312-4c9a-8162-60a942fd33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Thymio import Thymio\n",
    "th = Thymio.serial(port=\"COM7\", refreshing_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d7c0e-3758-482b-a95e-fce6a317674d",
   "metadata": {},
   "source": [
    "## Main implementation to run the project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get map information and Initialise Vision's instance\n",
    "Vision=Vision_Agent()\n",
    "Vision.read_save_image(\"image_start.png\")\n",
    "Vision.update() # get robot , obstacles and goals positions\n",
    "#Initialize the robot's instance\n",
    "myRobot=utils.MyThymio(th, Vision.center_robot[0],Vision.center_robot[1],utils.deg_to_rad(Vision.angle))\n",
    "#Initialize Kalman\n",
    "Kalman.kalman_init(Vision)\n",
    "#Initialize Variables\n",
    "checkPointReached=0\n",
    "finalGoalReached=0\n",
    "goal=Vision.objective_green\n",
    "\n",
    "# Main loop\n",
    "while True\n",
    "    # construct/reconstruct the path to the goal\n",
    "    Vision.path = run_A_Star(Vision.parcours, myRobot.get_pos(), goal)\n",
    "    Vision.print_path() \n",
    "    \n",
    "    goalReached=navigation.follow_path(myRobot,Vision,Vision.path)\n",
    "    \n",
    "    if goalReached :\n",
    "        if checkPointReached == 0 :\n",
    "            checkPointReached=1\n",
    "            goal=Vision.objective_red\n",
    "        else :\n",
    "            print(\"Final destination reached\")\n",
    "            myRobot.motor_stop()\n",
    "            break\n",
    "            \n",
    "Vision.cam.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
