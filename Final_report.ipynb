{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ff16be-b75d-4bab-9167-c2c45d300ffa",
   "metadata": {},
   "source": [
    "# Report Basics of Mobile Robotics  \n",
    "## Students: \n",
    "- Levy Alexandre \n",
    "- Tourki Amine \n",
    "- Tourki Emna \n",
    "- Zghari Aya\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07319473-9f87-4067-8566-ea967919f7c0",
   "metadata": {},
   "source": [
    "Link to our [Github](https://github.com/AmineTourki/Mobile-Robotics-)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1bf413-0f4a-44e5-ab78-d5972213dfd8",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project aims to make a Thymio navigate in a course where it will have to use Computer Vision, Global and Local Navigation with a filter to reduce positioning errors.\n",
    "\n",
    "The arena that we present you is modular, the obstacles can change place and the start and goal can be completely different. In this arena we also force our robot to pass through a checkpoint, which will spice up the route it has to make.The Thymio first goes to the checkpoint in green and then to the final point in red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad93698-472b-4be3-bcf8-50ed8256e11e",
   "metadata": {},
   "source": [
    "## Method Overview \n",
    "![alt text](report_images/Arena_axis.png \"Title\") \n",
    "**Main ideas:**\n",
    "- The \"global obstacles\" are the black 2D rectangles on the ground detected by the vision.\n",
    "- The \"Local obstacles\" are physical 3D obstacles detected by the proximity sensors and will be intoduced while the robot moves along the global path. We chose a cylindrical white obstacle for the presentation.  \n",
    "- Our robot cannot move through an arc of circle.It can only move straight forward or rotate in place. However , the navigation can still be smooth if we use an 8-Direction A_Star path planning and take small steps forward.That's why in this project we used an 8-Direction A_Star path planning and we set a small fixed time step to go forward.  \n",
    "\n",
    "GLOBAL_FORWARD = 0.1  # step time (in seconds) spent going forward in global navigation.  \n",
    "LOCAL_FORWARD = 0.1 # step time (in seconds) spent going forward in local navigation.\n",
    "\n",
    "- We chose a speed of MOTORSPEED = 50 for both the rotation and and the forward movement.\n",
    "- We used a Kalman filter.We use simple odometry to estimate the robot position and the camera to correct the robot position.Thus, the camera will be used continuously.\n",
    "- The global axes are defined as in the image above and the absolute angle of the robot is then defined positive if clockwise. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d33d7-b761-4a80-aa57-d39f4aeb422d",
   "metadata": {},
   "source": [
    "## Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5325f061-c856-4cf4-955d-06f984c0788d",
   "metadata": {},
   "source": [
    "### Map Setup\n",
    "\n",
    "<img src=\"Images/Setup.png\" width=\"600\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c3080c-27a7-41d6-808a-1bd7a5afb7dc",
   "metadata": {},
   "source": [
    "### Implementation details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ea71e-f236-44ef-86c9-4fd1dab139ae",
   "metadata": {},
   "source": [
    "The vision Class is strongly dependent on the map data, which can be updated continuously from the video. The construction of a global map consists of the following steps:\n",
    "\n",
    "1. The raw images are sampled at a given frequency from the webcam.\n",
    "2. A rebounding of the arena is done.\n",
    "3. Multiple color masks are applied to the transformed image to distinguish the objectives,the thymio, the arena and obstacles.\n",
    "4. Multiple color masks are applied to the transformed image to distinguish the target position, thymio (front and rear) and obstacles.\n",
    "\n",
    "Note:\n",
    "The rebounding and the localisation of the objectives and the obstacle are done on the first image. The next images are used for the robot position given to the Kalman filter.   \n",
    "The open library CV2 was used for the vision module.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016e7f86-3c09-425d-8465-ce9ea042df97",
   "metadata": {},
   "source": [
    "- Input\n",
    "    \n",
    "    - Raw image captured from camera\n",
    "    - Distance in real between the two circles on the robot to have a real world scale\n",
    "    - Color HSV values that helps define the map (this is sensitive to light and needs to be calibrated after setting environment)\n",
    "    \n",
    "- Output\n",
    "    - Global map that marks all occupancy with 1 and vacanncy with 0\n",
    "    - Starting pose, checkpoint pose and finish pose of the Thymio\n",
    "\n",
    "- Limitations \n",
    "    - The color masks are rather sensitive to lighting conditions and hyperparameters had to be calibrated\n",
    "    \n",
    "- Keyparameters\n",
    "    - Color mask Table\n",
    "        | Color | Used for | Lower Bounds | Upper Bounds |\n",
    "        | --- | --- | --- | --- |\n",
    "        | Blue | Robot Position | [87,97,50] | [164,255,160] |\n",
    "        | Red | Final Destination | [150,70,50] | [190,255,255] |\n",
    "        | Green | Checkpoint | [40,40,40] | [100,255,255] |\n",
    "        | Black | Robot Position | [0,0,0] | [180,255,50] |\n",
    "        | White+Green+Blue+Red | Reboarder Map |  [0,0,40] | [255,255,255] |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba377d8-0613-492b-829b-90ca5f057f79",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Class Vision\n",
    "\n",
    "Here is the class that can be found in vision_agent.py and its initialization:\n",
    "\n",
    "    class Vision_Agent:\n",
    "    def __init__(self):\n",
    "        self.converter = 0\n",
    "        self.cam = cv2.VideoCapture(1)\n",
    "        ret, frame = self.cam.read()  # initialize\n",
    "        self.image = frame\n",
    "        self.resize()                 #resize the image\n",
    "        self.hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV) # convert to HSV\n",
    "        self.get_first_image_cam(self.image)                   # delimit of the image\n",
    "        self.hsv = cv2.cvtColor(self.image, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        \n",
    "        self.angle = 0              # angle of the robot\n",
    "        self.center_robot = [0,0]   # center of the robot\n",
    "        self.parcours = np.zeros((50,50), np.uint8) # occupancy grid\n",
    "        self.parcours_2_pix=[len(self.image[0])/len(self.parcours[0]),len(self.image)/len(self.parcours)]\n",
    "        self.objective_red = None\n",
    "        self.objective_green = None\n",
    "        self.r_in_pix = 0\n",
    "        self.r_in_real = 6\n",
    "\n",
    "The Object Vision is going to initialise all these values and we are going to call update methods and at any time, there are also methods for visualisation and others returning  coefficient values like the conversion from pixel to a real value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1229518-8474-4fc1-8bf7-75d899368ac5",
   "metadata": {},
   "source": [
    "Here is a photo example to illustrate the vision code : \n",
    "\n",
    "<img src=\"Images/Image13.jpeg\" width=\"220\"/>\n",
    "\n",
    "In order to output a global map, each different object has to be detected. In order to detect the robot and its direction we put two blue rounds on it with different sizes, the small one is pointing in its direction. The black objects are the obstacles. The red represents the destination point. The green represents a checkpoint. \n",
    "\n",
    "#### Methods in the class\n",
    "\n",
    "##### resize()\n",
    "\n",
    "Resize the image returns by the camera so that the computation time is smaller. \n",
    "\n",
    "##### read_image()\n",
    "\n",
    "Read a new image from the camera.\n",
    "\n",
    "##### mask_thresh (image, thresh_low, thresh_high)\n",
    "\n",
    "Make a mask of the image in the range specified by the thresholds.\n",
    "\n",
    "#### redefine_mask\n",
    "\n",
    "Apply Morphological Operations so that erases noise in the mask.\n",
    "\n",
    "\n",
    "<img src=\"Images/Denoise.png\" width=\"220\"/>\n",
    "<img src=\"Images/Boom.png\" width=\"220\"/>\n",
    "\n",
    "##### get_robot()\n",
    "\n",
    "Get the position of the robot in the image stored.\n",
    "\n",
    "Here is a photo of the mask after mask_thresh with black and redefine mask for denoising\n",
    "\n",
    "![alt text](Images/Mask_circles.png \"Title\")\n",
    "\n",
    "Then we detect the circles with cv2 HoughCircle and Compute their centers\n",
    "\n",
    "![alt text](Images/Image_Robot.png \"Title\")\n",
    "\n",
    "##### get_obstacles()\n",
    "\n",
    "Get the position of the bounding box of the obstacles and returns an occupancy grid for the A* algorithm\n",
    "\n",
    "\n",
    "\n",
    "Here are photos of the masks after mask_thresh with black and redefine mask for denoising  \n",
    "![alt text](Images/Mask_obstacles.png \"Title\")\n",
    "\n",
    "Then we create a bounding box around each obstacle  \n",
    "![alt text](Images/Image_obstacles.png \"Title\")\n",
    "\n",
    "This parcours is returned as occupancy grid and the obstacles are also dilated so it is easier to dodge them   \n",
    "<img src=\"Images/parcours_a_star.png\" width=\"220\"/>\n",
    "\n",
    "##### get_objectives()\n",
    "\n",
    "Get the center of the objective.\n",
    "\n",
    "Here is a photo of the mask after mask_thresh with green and red and redefine mask for denoising  \n",
    "![alt text](Images/Mask_objective_green.png \"Title\")\n",
    "![alt text](Images/Mask_objective_red.png \"Title\")\n",
    "\n",
    "Then we create a bounding box around each objective and get their center  \n",
    "![alt text](Images/Image_Objectives.png \"Title\")\n",
    "\n",
    "\n",
    "##### print_path()\n",
    "\n",
    "Print the path generated by the A* algorithm on the image.  \n",
    "\n",
    "<img src=\"Images/Image50.jpeg\" width=\"220\"/>\n",
    "\n",
    "##### get_first_image_cam()\n",
    "\n",
    "Crop the image so that it fits the arena and store the coordinates of the arena.\n",
    " \n",
    "It first creates a mask of the arena with mask_thresh and morphological Operations.  \n",
    "<img src=\"Images/Image_reboardered.png\" width=\"220\"/>\n",
    "\n",
    "Then we detect the angles of this mask and recompute a reboardered image from the angles\n",
    "\n",
    "##### get_image_cam()\n",
    "\n",
    "Use the coordinates of the arena to crop the Image .\n",
    "\n",
    "##### get_pix_2_real()\n",
    "\n",
    "Returns a factor to convert from pixel value to real value in cm.\n",
    "\n",
    "##### get_grid_2_real ()\n",
    "\n",
    "Returns a factor to convert from grid value to real value in cm.\n",
    "\n",
    "##### update\n",
    "\n",
    "Do  get_robot(), get_obstacles(), get_objectives(). All the necessary task to detect all the object on the map.\n",
    "\n",
    "\n",
    "![alt text](Images/Image.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb1ad5e-2a86-4ac6-b666-c9167e494324",
   "metadata": {},
   "source": [
    "## Navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350bab49-9f03-4988-a122-e0196a04b28c",
   "metadata": {},
   "source": [
    "To facilitate the navigation, we created a class that represents the robot and control its motors. The class can be found in utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856344be-aca4-46aa-aea9-4a87d1abd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyThymio(object):\n",
    "\n",
    "    def __init__(self, th, x, y, theta):\n",
    "        \"\"\"\n",
    "        Initialisation of the object\n",
    "        :param th: instance linking to the Thymio robot connected via USB\n",
    "               x,y: x and y positions of the center of the Thymio\n",
    "               theta: the absolute angle orientation of the Thymio\n",
    "        :return: creates the object of class MyThymio\n",
    "        \"\"\"\n",
    "\n",
    "        self.th = th\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.angle = normalise_angle(theta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfd1bb6-33c7-47f2-b6c5-5c7f5e36598e",
   "metadata": {},
   "source": [
    "Here are the methods that we can call on an instance: \n",
    "- def get_pos(self)\n",
    "- def set_pos(self, x, y)\n",
    "- def motor_stop(self)\n",
    "- def motor_forward(self, time_forward)\n",
    "- def motor_rotate(self, dtheta)\n",
    "- def get_angle(self)\n",
    "- def set_angle(self, theta)\n",
    "- def get_previous_pos(self)\n",
    "\n",
    "*Notes:*  \n",
    "\n",
    "- The method **motor_rotate(self, dtheta)** sets the speed of the motors to ±MOTORSPEED to make the robot turn on himself dtheta rad then updates self.angle. It takes into consideration the sign of dtheta. If dtheta positive/ negative, the robot turns right/left.\n",
    "- The method **motor_forward(self, time_forward)** sets the speed of both motors to MOTORSPEED  for time_forward seconds.However this method don't update the positions x,y of the instance self. x,y of the instance self are only updated after estimating the true positions with the Kalman filter.They are updated each time the function **callKalman** is called.The definition of this function **callKalman** is in the file navigation.py\n",
    "- The method **get_previous_pos(self)** is used in the function **callKalman**. Since x and y are only updated just after calling kalman, they are the previous positions of our robot when calling it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832ff2b4-9b43-4f5d-ab2a-0b61de1747b9",
   "metadata": {},
   "source": [
    "### A-Star Path Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1617b2d4-b461-4ea5-8f0e-fb12fc16f356",
   "metadata": {},
   "source": [
    "We use the same A* algorithm provied in the exercise session (session 5: Path planning). It is based on the pseudocode provided in [Wikipedia](https://en.wikipedia.org/wiki/A*_search_algorithm#Pseudocode). \n",
    "The vision module is responsible for calculating the occupancy grid, the size of the grid is modular and can be adjusted in the Vision module. For our project, we used a grid size of 50  by 50 for performance reasons (A 50 by 50 grid takes around 4 seconds to be resolved while a grid of 60 by 60 takes 12 seconds). For a better looking and smoother navigation, we also opted for a 8-connectivity movement (8N)  \n",
    "**Note:** The implementation of the code can be found in the file A_star_algorithm.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c52403b-91b9-410c-a26b-3a1957b919e7",
   "metadata": {},
   "source": [
    "We created a plotting function to draw the occupancy grid and the path generated by the algorithm and then save a picture locally. The picture is then ploted in order overview the robot's path. The plotting function was added to the same file as the A* algorithm and is used directly after generating the path. It has the following signature: \n",
    "\n",
    "```Python\n",
    "def plot_path(occupancy_grid,visited_nodes,start,goal,path)\n",
    "```\n",
    "\n",
    "\n",
    "The A* algorithm is very dependant on the grid generated by the vision (as explained above). Here is an example of how the algorithm works with our arena. The Vision module starts by taking a picture of the Arena.\n",
    "\n",
    "<img src=\"report_images/A_star_0.jpeg\"/>\n",
    "The vision would then generate the following mask.\n",
    "<img src=\"report_images/A_star_1.png\"/>  \n",
    "That mask is used to create the occupancy grid that is forwarded to the A* algorithm. The occupancy grid is of size 50x50 in our project. The grid is binary, a 0 means the path is free and a 1 means the path is blocked. The following grid is given to the A* algorithm.\n",
    "<img src=\"report_images/A_star_2.png\"/>  \n",
    "Provided with that grid and with the position of the robot and the first goal (checkpoint in green), The algorithm generates the following path and visited nodes.\n",
    "<img src=\"report_images/visited_green.png\"/>\n",
    "The robot then navigates towards it's first goal by following the green path. Once the green objective is reached, we use the A* algorithm again to find a path to the red objective (Final goal).\n",
    "<img src=\"report_images/visited_red.png\"/>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c806ca-cda8-43c7-8634-8734ac118fb0",
   "metadata": {},
   "source": [
    "The following function was created to directly call the A star algorithm, it only takes as input the Start, the Goal and the occupency grid. The plotting function mentionned above is also called in this function. At the end of the call, this function would then return a path for the robot in the form of a list and save a plot of the path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b880b94b-b264-47a5-9c47-ebc0cc87dc32",
   "metadata": {},
   "source": [
    "```Python\n",
    "def run_A_Star(occupancy_grid, start, goal)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96fc39f3-3e22-4fe3-9100-126809cfacd1",
   "metadata": {},
   "source": [
    "The A* algorithm is called at least twice, once for every path (path to the green checkpoint and then path to the red goal). We also call the algorithm every time the robot performs local navigation to avoid an obstacle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4421c4",
   "metadata": {},
   "source": [
    "### Global navigation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08588e6a",
   "metadata": {},
   "source": [
    "The main loop (see implementation at the end of the report) follows this pseudo code:  \n",
    "\n",
    "Repeat  \n",
    "- construct path using function **run_A_Star**\n",
    "- follow that path using function **navigation.follow_path**. Two possibilities to exit **the function follow path**:\n",
    "    - First one: The robot follow the path until the end. The goal is reached. Exit **navigation.follow_path** with return True.\n",
    "    - Second one: When following the path, a local obstacle is detected with the proximity sensors. The robot enters the local navigation , moves to avoid the obstacle locally then exit **navigation.follow_path** with return False to construct a new path with the A_Star.\n",
    "- If first goal objective_green reached (checkpoint goal), change goal to objective_red (final destination)\n",
    "- If red goal is reached, break the loop\n",
    "        \n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bf2835",
   "metadata": {},
   "source": [
    "Here are the function **navigation.follow_path** and the functions related to it. The definition of these function are imported from  the file navigation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba960848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def closest_node_index(current_pos, path, old_index):\n",
    "    \"\"\"\n",
    "    return the closest node of the path to the current position of the robot\n",
    "    and if the robot should change his orientation\n",
    "\n",
    "    :param current_pos: current position of the robot\n",
    "    :param path: array containing the optimal path to follow\n",
    "    :param old_index: index taken in the last call of this function\n",
    "    :return:    closestnode_index (int)\n",
    "                change_angle (bool)\n",
    "\n",
    "    \"\"\"\n",
    "    # check the nodes from old index and ahead and get the index of the closest node to our current position\n",
    "    closestnode_index = old_index + np.linalg.norm(current_pos - path[old_index:], axis=1).argmin()\n",
    "    # only change the orientation if the robot is closest to one of his next nodes compared to the initial node\n",
    "    # don't change angle if already close to goal (goal index ==len(path) - 1)\n",
    "    change_angle = (closestnode_index != old_index) and (closestnode_index != len(path) - 1)\n",
    "    return change_angle, closestnode_index\n",
    "\n",
    "\n",
    "def change_orientation(myRobot, next_pos):\n",
    "    \"\"\"\n",
    "    adjust the orientation of the robot by rotating him towards his next_pos\n",
    "    :param myRobot: our robot, instance of Class MyThymio\n",
    "    :param next_pos: coordinate of the next position\n",
    "    \"\"\"\n",
    "    target_vector = next_pos - myRobot.get_pos()\n",
    "    dtheta = math.atan2(target_vector[1], target_vector[0]) - myRobot.get_angle()\n",
    "    # rotate the robot with angle dtheta\n",
    "    myRobot.motor_rotate(dtheta)\n",
    "\n",
    "\n",
    "def follow_path(myRobot, Vision, path):\n",
    "    \"\"\"\n",
    "    Follow the path until the goal is reached or until the robot enters local navigation\n",
    "    :param myRobot: our robot, instance of Class MyThymio\n",
    "    :param Vision: our vision agent, instance of Class Vision_Agent\n",
    "    :param path: array containing the optimal path to follow\n",
    "    :return:    True if the goal is reached\n",
    "                False if the robot entered local navigation (goal not reached)\n",
    "    \"\"\"\n",
    "    closest_index = 0\n",
    "    first_step = 1\n",
    "    while math.dist(myRobot.get_pos(), path[-1]) > EPS_GOAL:  # path[-1] is the goal\n",
    "        change_angle, closest_index = closest_node_index(myRobot.get_pos(), path, closest_index)\n",
    "        # only change the orientation if the robot is closest to his next node compared to the initial node\n",
    "        if change_angle or first_step:\n",
    "            first_step = 0\n",
    "            next_pos = path[closest_index + 1]\n",
    "            # rotate to face the new target node next_pos\n",
    "            change_orientation(myRobot, next_pos)\n",
    "        # go forward GLOBAL_FORWARD(0.1) seconds\n",
    "        myRobot.motor_forward(GLOBAL_FORWARD)\n",
    "        # estimate new position and update myRobot pose\n",
    "        callKalman(myRobot, Vision, GLOBAL_FORWARD)\n",
    "        # enter local navigation if obstacle detected\n",
    "        entered_local_navigation = avoid_obstacle(myRobot, Vision)\n",
    "        if entered_local_navigation:\n",
    "            # if the robot entered local navigation , return to the main implementation loop to construct a new path\n",
    "            return False\n",
    "\n",
    "    print(\"Goal reached\")\n",
    "    myRobot.motor_stop()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859dee0",
   "metadata": {},
   "source": [
    "*Notes:*  \n",
    "\n",
    "- In the function **closest_node_index(current_pos, path, old_index)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66248219-6c45-4c5d-9639-2d5cc2e85b5e",
   "metadata": {},
   "source": [
    "## Local Navigation\n",
    "For local navigation, we use the front proximity sensors to detect when an obstacle is locally in range. When a threshold is reached, the robot enters an obstacle avoidance loop until the object is passed.  \n",
    "**Note**: The implementation of the local navigation can be found in the file **local_navigation.py**  \n",
    "\n",
    "The local avoidance is done by repeating the following steps: \n",
    "1. Read the proximity sensors \n",
    "2. verify that the robot is still in range of the obstacle\n",
    "3. calculate the angle of rotation\n",
    "4. rotate the robot and go forward for a fixed period of time\n",
    "5. rectify position with Kalman\n",
    "\n",
    "### 2. Verify that the robot is still in range of the obstacle\n",
    "This part is very straightforward. We simply use a threshold value of 2500 and whenever one of the sensors detects an equal or higher value, we consider that the robot is the range of an obstacle.  \n",
    "The value OBSTACLE_TH_MIN = 2500 was calculated experimentally and it's the one that best suits the wanted reaction given the lightning in our setup.  \n",
    "The function that verifies this condition is the following:\n",
    "\n",
    "```Python\n",
    "def detect_obstacle(prox_sensors)\n",
    "```\n",
    "### 3. Calculate the angle of rotation\n",
    "In order to calculate the angle of rotation, we consider each side (right/left) of the robot seperately. The side with the highest sensor readings wins and is responsible for the calculation of the angle of rotation (the right side is represented by the sum of sensors 3 and 4 while the left side is represented by the sum of sensor 0 and 1)  \n",
    "After choosing the side, we calculate the angle as a weighted sum of fixed rotation angles. We assigned the following rotation angles to each sensor:  \n",
    "\n",
    "Sensor | Angle of rotation\n",
    ":-:|:-:\n",
    "Front sensor|60°\n",
    "Middle sensor|45°\n",
    "Side snesor|20°  \n",
    "\n",
    "\n",
    "  \n",
    "The front sensor is always the proximity sensor 2, while the middle is 3 or 1 depending on the side and the side sensor is also 0 or 4 depending on the side.  \n",
    "To calculate the angle we first sum the readings of all the sensors:  \n",
    "\n",
    "\\begin{equation}\n",
    "sum = side{\\_}sensor + middle{\\_}sensor + front{\\_}sensor\n",
    "\\end{equation}\n",
    "\n",
    "The rotation angle is then:\n",
    "\n",
    "\\begin{equation}\n",
    "angle = \\frac{side{\\_}sensor}{sum}*20°+\\frac{middle{\\_}sensor}{sum}*45°+\\frac{front{\\_}sensor}{sum}*60°\n",
    "\\end{equation}  \n",
    "\n",
    "\n",
    "The angle of rotation is then bounded between 20° and 60°.  \n",
    "The function that calculates the angle of rotation and chooses which side to rotate to is the following:  \n",
    "```Python\n",
    "def rotation_angle (prox_sensors)\n",
    "```  \n",
    "\n",
    "\n",
    "### 4. Rotate the robot and go forward for a fixed period of time\n",
    "After rotating the robot, we go forward by a fixed amount of time. The selected time in this project is 0.2s , this value best fits our project and was selected experimentally. Testing with lower values would lead the robot to start shaking and higher values would cause the robot to not react in time and go into the obstacle.  \n",
    "\n",
    "### 5. Rectify position with Kalman\n",
    "Just like the global navigation, after moving forward, the robot calls Kalman to rectify its position with the help of the Vision agent.    \n",
    "The final function that implements all the local navigation is the following  \n",
    "```Python\n",
    "def avoid_obstacle(myRobot,Vision)\n",
    "``` \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b75f97a-46d4-4a7c-81f6-3da93456dc23",
   "metadata": {},
   "source": [
    "## Filter\n",
    "### Kalman Filter \n",
    "(This Kalman filter was designed using the solution provided in exercise session 8)     \n",
    "As per usual, this Kalman filter consists of a prediction step and a correction step. We use simple odometry to estimate the robot position and the camera to correct the robot position. The filter was also designed to work on linear movements only.  \n",
    "The global navigation consists of translation steps and rotations steps done seperately, but we will assume that the rotations do not infer any error in the odometry. This choice was made because the camera is not sensitive enough to detect small rotations and would return false values. We also found that using only the translation is sufficient and the filter performs still well. We also use a constant speed of 50 for the Thymio.\n",
    "The filter takes the position of the robot as well as its orientation as an input vector.  \n",
    "All the calculus was made in centimeters for the robot positions and in radians for the angles  \n",
    "**Note:** the implementation of the Kalman filter can be found in the file Kalman.py\n",
    "\n",
    "#### Variables\n",
    "Here is a list of the matrices and vectors used in the Kalman filter  \n",
    "\n",
    "Variable | Description\n",
    ":-:|:-:\n",
    "X|Filter state estimate (vector of size 3)\n",
    "P|Covariance matrix (3x3 matrix)\n",
    "A|Dynamic matrix (3x3 matrix)\n",
    "Q|Position error/uncertainty (3x3 matrix)\n",
    "R|Vision uncertainty/noise (3x3 matrix)\n",
    "H|Measurement function (3x3 matrix)\n",
    "\n",
    "\n",
    "As said before, the State vector contains the position of the robot x,y and his orientation.  \n",
    "\n",
    "$$X = \\begin{bmatrix} x\\\\\n",
    "y\\\\\n",
    "\\alpha\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "We takes an arbitrary covariance matrix with high values as we are uncertain of where the robot starts\n",
    "\n",
    "$$P = \\begin{bmatrix} 100 && 0 && 0\\\\\n",
    "0 && 100 && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{4}\n",
    "\\end{bmatrix}$$  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23823a7a-98e7-4302-921c-7731dab6ddca",
   "metadata": {},
   "source": [
    "The odometry uses this dynamic equation:  \n",
    "\n",
    "$$X_{priori}=\\begin{bmatrix} x_{priori}\\\\\n",
    "y_{priori}\\\\\n",
    "\\alpha_{priori}\n",
    "\\end{bmatrix} = \\begin{bmatrix} x_{previous}\\\\\n",
    "y_{previous}\\\\\n",
    "\\alpha_{previous}\n",
    "\\end{bmatrix} + \n",
    "\\begin{bmatrix} speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "speed*\\Delta t*cos(\\alpha_{previous})\\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0e8f3a-2cba-4aa1-98fe-1fa6f170c79f",
   "metadata": {},
   "source": [
    "For the computation of the new covariance matrix, we take the jacobian of the dynamic matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600ba3a1-329e-4c59-8bef-c8bdc5aca40c",
   "metadata": {},
   "source": [
    "$$A = \\begin{bmatrix} 1 && 0 && -speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "0 && 1 && -speed*\\Delta t*sin(\\alpha_{previous})\\\\\n",
    "0 && 0 && 0\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fb3ead-0ff9-476c-8c88-5eb2eaafb974",
   "metadata": {},
   "source": [
    "and we finally add the uncertainty matrix to our covariance matrix. we obtain:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d6d19a-8aa9-42a6-ba0f-0494864b8e0b",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "P_{priori}=APA^{T} + Q\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98973d8-3fe9-47f5-a2c5-1748c093ddd6",
   "metadata": {},
   "source": [
    "For the matrix Q we use the variance of the robot speed to calculate a position variance using sampling, we obtain a value of  $\\sigma_x$ = $\\sigma_y$ = 0.123 cm. The error on the angle is set arbitrarily to $3^{\\circ}$ = $\\frac{\\pi}{60}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7283ff66-1dde-435c-843d-bcaa3f91f841",
   "metadata": {},
   "source": [
    "$$Q = \\begin{bmatrix} 0.123 && 0 && 0\\\\\n",
    "0 && 0.123 && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{60}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09a79b3-13a6-432d-bfc8-6331cf061fe4",
   "metadata": {},
   "source": [
    "Once the estimation step is complete, we use the update step to correct the robot position using the robot position returned by the camera.  \n",
    "In this step we simply assume the noise R on the camera readings to be equal to $\\sigma$ = 2*pixel size (cm). The value of the pixel size in centimeters is given by the the vision agent. We obtain diffent values of pixel size for the x and y values depending on the size of the Arena we are using. We also approximate the angle error on the camera to $4.5^{\\circ}$ = $\\frac{\\pi}{40}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40468c8e-f532-4009-9ff6-bffaf4ec1805",
   "metadata": {},
   "source": [
    "$$R = \\begin{bmatrix} \\sigma_x && 0 && 0\\\\\n",
    "0 && \\sigma_y && 0\\\\\n",
    "0 && 0 && \\frac{\\pi}{40}\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f64b17a-32db-4adb-bfa6-8922359b7ffe",
   "metadata": {},
   "source": [
    "The measurement matrix H is a unit diagonal matrix as we simply take the values returned by the vision module without modifications. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2def79b1-6252-4877-9400-d1aa52e11419",
   "metadata": {},
   "source": [
    "$$H = \\begin{bmatrix} 1 && 0 && 0\\\\\n",
    "0 && 1 && 0\\\\\n",
    "0 && 0 && 1\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c238f8-616b-4186-886c-620d1aff8666",
   "metadata": {},
   "source": [
    "We then simply calculate the innovation $i$ and the prediction covariance S in order to calculate the Kalman gain K. The equations are as follow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f26bc-39bf-49a7-a3e7-afb5cca81bc2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "i = X_{mesure} - HX_{priori}  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "S=HP_{priori}H^{T} + R\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18814af8-14b5-4aae-8583-5125a6802513",
   "metadata": {},
   "source": [
    "The Kalman K gain tells how much the predictions should be corrected based on the measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e6d007-69f4-4692-b511-c36b31d9e4c2",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "K=P_{priori}H^{T}S^{-1}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b0e852-fb1e-4a85-a46d-94387d534b30",
   "metadata": {},
   "source": [
    "We can then calculate a posterior position estimate and a posterior covariance matrix estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11ab318-d469-48da-827b-0b2c502e3413",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "X_{post} = X_{mesure} + ki  \n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "P_{post}=P_{priori} - KHP_{priori}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24f1c0b-ceda-4efb-9034-7d540b2592de",
   "metadata": {},
   "source": [
    "Given that the image quality of the camera isn't very high, we can get errors on the measured values at random times. To make up for that we simply discard the measured values by the camera and only use the estimated values in the Kalman filter. A boolean variable (robot_detected) is simply set to True by the vision module in order for the Kalman filter to use the prediction only "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edfbb5d-b57e-4432-9572-c28a682b3c50",
   "metadata": {},
   "source": [
    "The final Kalman filter can be called by the function: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b58316c-4607-457c-b2c5-cc5bf0c03209",
   "metadata": {},
   "source": [
    "```Python\n",
    "def kalman_estimate(X_previous,P_previous,delta_t,speed)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed899a1b-c23d-4983-ab3f-98fd20310049",
   "metadata": {},
   "source": [
    "#### Comparison with and without filter\n",
    "Above a comparison of the navigation when the Kalman filter is active (with vision) and when the Kalman filter is inactive (estimation only)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4f28cc-3eb3-48db-8cad-d7ea623d9202",
   "metadata": {},
   "source": [
    "**With Kalman** | **Without Kalman**\n",
    ":-:|:-:\n",
    "<img src=\"report_images/gif_with_kalman.gif\" width=\"450\"/>|<img src=\"report_images/gif_without_kalman.gif\" width=\"418\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474b5dad",
   "metadata": {},
   "source": [
    "## Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eff52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math as m\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), 'src'))\n",
    "from vision_agent import Vision_Agent\n",
    "import local_navigation\n",
    "from A_star_algorithm import run_A_Star\n",
    "import navigation\n",
    "import utils\n",
    "import  Kalman\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913344fa-45f6-4c8c-a706-a2285c1073ad",
   "metadata": {},
   "source": [
    "## Connexion to Thymio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9f069b-b312-4c9a-8162-60a942fd33bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Thymio import Thymio\n",
    "th = Thymio.serial(port=\"COM7\", refreshing_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9d7c0e-3758-482b-a95e-fce6a317674d",
   "metadata": {},
   "source": [
    "## Main implementation to run the project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4432d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get map information and Initialise Vision's instance\n",
    "Vision=Vision_Agent()\n",
    "Vision.read_save_image(\"image_start.png\")\n",
    "Vision.update() # get robot , obstacles and goals positions\n",
    "#Initialize the robot's instance\n",
    "myRobot=utils.MyThymio(th, Vision.center_robot[0],Vision.center_robot[1],utils.deg_to_rad(Vision.angle))\n",
    "#Initialize Kalman\n",
    "Kalman.kalman_init(Vision)\n",
    "#Initialize Variables\n",
    "checkPointReached=0\n",
    "finalGoalReached=0\n",
    "goal=Vision.objective_green\n",
    "\n",
    "# Main loop\n",
    "while True\n",
    "    # construct/reconstruct the path to the goal\n",
    "    Vision.path = run_A_Star(Vision.parcours, myRobot.get_pos(), goal)\n",
    "    Vision.print_path() \n",
    "    \n",
    "    goalReached=navigation.follow_path(myRobot,Vision,Vision.path)\n",
    "    \n",
    "    if goalReached :\n",
    "        if checkPointReached == 0 :\n",
    "            checkPointReached=1\n",
    "            goal=Vision.objective_red\n",
    "        else :\n",
    "            print(\"Final destination reached\")\n",
    "            myRobot.motor_stop()\n",
    "            break\n",
    "            \n",
    "Vision.cam.release()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
